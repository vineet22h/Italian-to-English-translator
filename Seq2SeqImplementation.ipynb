{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwPL0hIlGKoA"
   },
   "source": [
    "# <font color='red'>**Sequence to sequence implementation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyfZo8fmLOec"
   },
   "source": [
    "## 1: Simple Encoder and Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3k_AlAuKJqVA"
   },
   "source": [
    "<font color='blue'>**Load the data**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "Qln8nZgrgxZX"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "KztMNY5dgxZX"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fU80Ao-AGaob",
    "outputId": "0110ff5d-d945-4930-ec31-c2cf5442f0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341555\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "data = open('ita.txt', 'r', encoding = 'utf-8').read().split('\\n')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "cQwlPf-HgxZY",
    "outputId": "fcae332b-13c9-4816-a75e-4c70524fcdc5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>italin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Ciao!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Corri!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Corra!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Correte!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Chi?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english    italin\n",
       "0     Hi.     Ciao!\n",
       "1    Run!    Corri!\n",
       "2    Run!    Corra!\n",
       "3    Run!  Correte!\n",
       "4    Who?      Chi?"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = []\n",
    "for i in range(100000):\n",
    "    try:\n",
    "        english, italin = data[i].split('\\t')[0:2]\n",
    "        df.append([english, italin])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame(df, columns = ['english', 'italin'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmGWTdRmKRph"
   },
   "source": [
    "<font color='blue'>**Preprocess data**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "9QqElB_nKZos",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "import re\n",
    "import numpy as np\n",
    "def preprocess(text):\n",
    "    # convert all the text into lower letters\n",
    "    # use this function to remove the contractions: https://gist.github.com/anandborad/d410a49a493b56dace4f814ab5325bbd\n",
    "    # remove all the spacial characters: except space ' '\n",
    "    text = text.lower()\n",
    "    for j, word in enumerate(text.split()):\n",
    "        try:\n",
    "    #         print(word)\n",
    "            if len(re.findall('[^\\w\\d\\ \"]', word))> 0:\n",
    "                text = re.sub(word, contractions.fix(word), text)\n",
    "        \n",
    "            text = re.sub('[^A-Za-z0-9èìò ]+', '', text)\n",
    "        except:\n",
    "            return np.nan\n",
    "    return text\n",
    "\n",
    "a = df.english.astype('str').apply(lambda x: preprocess(x))\n",
    "b = df.italin.astype('str').apply(lambda x: preprocess(x))\n",
    "df_preprocessed = pd.DataFrame(data = np.array([a, b]).T, columns = ['english', 'italin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8RDrP4xKabR"
   },
   "source": [
    "## <font color='blue'>**Implement custom encoder decoder**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A45uc0JILMlV"
   },
   "source": [
    "<font color='blue'>**Encoder**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "9cex2XfCLOew"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import tensorflow as tf\n",
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super().__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        #Initialize Embedding layer\n",
    "        self.enc_embed = Embedding(input_dim = inp_vocab_size, output_dim = embedding_size, input_length= input_length)\n",
    "        #Intialize Encoder LSTM layer\n",
    "        self.enc_lstm = LSTM(lstm_size, return_sequences = True, return_state = True)\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "        embedding = self.enc_embed(input_sequence)\n",
    "        output_state, enc_h, enc_c = self.enc_lstm(embedding, initial_state = states)\n",
    "        return output_state, enc_h, enc_c\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "        return [tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "x1ES1-sJLOe4"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "    def __init__(self,out_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super().__init__()\n",
    "        #Initialize Embedding layer\n",
    "        self.dec_embed = Embedding(input_dim = out_vocab_size, output_dim = embedding_size, input_length = input_length)\n",
    "        #Intialize Decoder LSTM layer\n",
    "        self.dec_lstm = LSTM(lstm_size, return_sequences = True, return_state = True)\n",
    "    \n",
    "    def call(self,input_sequence, initial_states):\n",
    "        embedding = self.dec_embed(input_sequence)\n",
    "        output_state, dec_h, dec_c = self.dec_lstm(embedding, initial_state = initial_states)\n",
    "        return output_state, dec_h, dec_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "BXrIj4scLOe_"
   },
   "outputs": [],
   "source": [
    "class Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self,*params):\n",
    "        super().__init__()\n",
    "        #Create encoder object\n",
    "        self.encoder = Encoder(inp_vocab_size = params[0], embedding_size = params[2], lstm_size = params[3], input_length = params[4])\n",
    "        #Create decoder object\n",
    "        self.decoder = Decoder(out_vocab_size = params[1], embedding_size = params[2], lstm_size = params[3], input_length = params[5])\n",
    "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
    "        self.dense = Dense(params[1], activation='softmax')\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, params, training = True):\n",
    "\n",
    "        enc_inp, dec_inp = params[0], params[1]\n",
    "        initial_state = self.encoder.initialize_states(batch_size)\n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        output, _, _ = self.decoder(dec_inp ,[enc_h, enc_c])\n",
    "        return self.dense(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "fpa-50eMgxZY"
   },
   "outputs": [],
   "source": [
    "df_preprocessed['english_inp'] = '<sos> '+df_preprocessed['english']\n",
    "df_preprocessed['english_out'] = df_preprocessed['english'] + ' <eos>'\n",
    "df_preprocessed['italin'] = df_preprocessed['italin'].apply(lambda x: str(x))\n",
    "df_preprocessed['italin'] = '<sos> '+df_preprocessed['italin']+' <eos>'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df_preprocessed, test_size = 0.1, random_state = 0)\n",
    "train, validation = train_test_split(train, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "GSQBdZ_MgxZY"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tknizer_ita = Tokenizer()\n",
    "tknizer_ita.fit_on_texts(train['italin'].values)\n",
    "tknizer_eng = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tknizer_eng.fit_on_texts(train['english_inp'].values + train['english_out'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "3O0gwOc9gxZY"
   },
   "outputs": [],
   "source": [
    "len_eng = df_preprocessed['italin'].astype(str).apply(lambda x: len(x))\n",
    "len_italin = df_preprocessed['english_inp'].astype(str).apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUuqbpsrgxZY",
    "outputId": "78efc9ae-0946-437a-cc28-d5c4ad512036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12.0\n",
      "10 24.0\n",
      "20 26.0\n",
      "30 28.0\n",
      "40 29.0\n",
      "50 31.0\n",
      "60 32.0\n",
      "70 34.0\n",
      "80 35.0\n",
      "90 38.0\n",
      "100 112.0\n",
      "90 38.0\n",
      "91 38.0\n",
      "92 39.0\n",
      "93 39.0\n",
      "94 40.0\n",
      "95 40.0\n",
      "96 41.0\n",
      "97 42.0\n",
      "98 43.0\n",
      "99 45.0\n",
      "100 112.0\n",
      "99.1 45.0\n",
      "99.2 46.0\n",
      "99.3 46.0\n",
      "99.4 47.0\n",
      "99.5 47.0\n",
      "99.6 48.0\n",
      "99.7 48.0\n",
      "99.8 50.0\n",
      "99.9 52.0\n",
      "100 112.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,101,10):\n",
    "    print(i,np.percentile(len_eng, i))\n",
    "for i in range(90,101):\n",
    "    print(i,np.percentile(len_eng, i))\n",
    "for i in [99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100]:\n",
    "    print(i,np.percentile(len_eng, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dstiA6igxZZ",
    "outputId": "461565de-2b05-4ad1-a9ca-5c8b0c0e1f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.0\n",
      "10 18.0\n",
      "20 20.0\n",
      "30 21.0\n",
      "40 22.0\n",
      "50 23.0\n",
      "60 24.0\n",
      "70 25.0\n",
      "80 25.0\n",
      "90 26.0\n",
      "100 45.0\n",
      "90 26.0\n",
      "91 26.0\n",
      "92 26.0\n",
      "93 26.0\n",
      "94 27.0\n",
      "95 27.0\n",
      "96 27.0\n",
      "97 27.0\n",
      "98 27.0\n",
      "99 28.0\n",
      "100 45.0\n",
      "99.1 28.0\n",
      "99.2 28.0\n",
      "99.3 28.0\n",
      "99.4 28.0\n",
      "99.5 28.0\n",
      "99.6 28.0\n",
      "99.7 28.0\n",
      "99.8 29.0\n",
      "99.9 29.0\n",
      "100 45.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,101,10):\n",
    "    print(i,np.percentile(len_italin, i))\n",
    "for i in range(90,101):\n",
    "    print(i,np.percentile(len_italin, i))\n",
    "for i in [99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100]:\n",
    "    print(i,np.percentile(len_italin, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "-KWbFCDqgxZZ"
   },
   "outputs": [],
   "source": [
    "max_len_eng = 0\n",
    "eng_vocab = set()\n",
    "for sent in df_preprocessed['english_inp'].values+df_preprocessed['english_out'].values:\n",
    "    length = len(sent.split())\n",
    "    if length > max_len_eng:\n",
    "        max_len_eng = length\n",
    "    for word in sent.split():\n",
    "        eng_vocab.add(word)\n",
    "eng_vocab_size = len(eng_vocab)\n",
    "\n",
    "max_len_ita = 0\n",
    "ita_vocab = set()\n",
    "for sent in df_preprocessed['italin']:\n",
    "    length = len(sent.split())\n",
    "    if length > max_len_ita:\n",
    "        max_len_ita = length\n",
    "    for word in sent.split():\n",
    "        ita_vocab.add(word)\n",
    "ita_vocab_size = len(ita_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "3t_F5U-BgxZZ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, data, tknizer_ita, tknizer_eng, max_len_ita, max_len_eng):\n",
    "        self.encoder_inps = data['italin'].values\n",
    "        self.decoder_inps = data['english_inp'].values\n",
    "        self.decoder_outs = data['english_out'].values\n",
    "        self.tknizer_eng = tknizer_eng\n",
    "        self.tknizer_ita = tknizer_ita\n",
    "        self.max_len_ita = max_len_ita\n",
    "        self.max_len_eng = max_len_eng\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        self.encoder_seq = self.tknizer_ita.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
    "        self.decoder_inp_seq = self.tknizer_eng.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_out_seq = self.tknizer_eng.texts_to_sequences([self.decoder_outs[i]])\n",
    "\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len_ita, dtype='int32', padding='post')\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len_eng, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len_eng, dtype='int32', padding='post')\n",
    "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)\n",
    "\n",
    "    \n",
    "class Dataloder(tf.keras.utils.Sequence):    \n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
    "#         print(batch[0].shape, batch[1].shape, batch[2].shape)\n",
    "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
    "        return tuple([[tf.convert_to_tensor(batch[0]), tf.convert_to_tensor(batch[1])], tf.convert_to_tensor(batch[2])])\n",
    "\n",
    "    def __len__(self):  # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bMWW4wpgxZZ",
    "outputId": "e8cb3957-a39e-4475-b218-bf57ed44ab04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('glove.6B.300d.txt', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "a9jKl976gxZZ"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((eng_vocab_size, 300))\n",
    "for word, i in tknizer_eng.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(ord)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "NZwYeDUugxZZ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import*\n",
    "import os\n",
    "import datetime\n",
    "batch_size=32\n",
    "lstm_size=128\n",
    "max_len_eng = 50\n",
    "max_len_ita = 29\n",
    "embedding_dim = 300\n",
    "dense_units = 256\n",
    "\n",
    "train_dataset = Dataset(train, tknizer_ita, tknizer_eng, max_len_ita, max_len_eng)\n",
    "test_dataset  = Dataset(test, tknizer_ita, tknizer_eng, max_len_ita, max_len_eng)\n",
    "val_dataset  = Dataset(validation, tknizer_ita, tknizer_eng, max_len_ita, max_len_eng)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=batch_size)\n",
    "val_dataloader = Dataloder(val_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLowAyTKeT5B",
    "outputId": "3d4e73db-af29-4cd0-bf8a-a0a5d581c2cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "   2/2531 [..............................] - ETA: 1:36:38 - loss: 10.1122WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1646s vs `on_train_batch_end` time: 4.4088s). Check your callbacks.\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.5123\n",
      "Epoch 00001: val_loss improved from inf to 0.33803, saving model to seq2seq\n",
      "2531/2531 [==============================] - 431s 170ms/step - loss: 0.5123 - val_loss: 0.3380\n",
      "Epoch 2/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.2979- ETA: 0s - loss: 0.29\n",
      "Epoch 00002: val_loss improved from 0.33803 to 0.26173, saving model to seq2seq\n",
      "2531/2531 [==============================] - 449s 178ms/step - loss: 0.2979 - val_loss: 0.2617\n",
      "Epoch 3/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.2302\n",
      "Epoch 00003: val_loss improved from 0.26173 to 0.20859, saving model to seq2seq\n",
      "2531/2531 [==============================] - 430s 170ms/step - loss: 0.2302 - val_loss: 0.2086\n",
      "Epoch 4/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.1780\n",
      "Epoch 00004: val_loss improved from 0.20859 to 0.16905, saving model to seq2seq\n",
      "2531/2531 [==============================] - 428s 169ms/step - loss: 0.1780 - val_loss: 0.1691\n",
      "Epoch 5/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.1369\n",
      "Epoch 00005: val_loss improved from 0.16905 to 0.13718, saving model to seq2seq\n",
      "2531/2531 [==============================] - 433s 171ms/step - loss: 0.1369 - val_loss: 0.1372\n",
      "Epoch 6/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 00006: val_loss improved from 0.13718 to 0.11437, saving model to seq2seq\n",
      "2531/2531 [==============================] - 429s 169ms/step - loss: 0.1035 - val_loss: 0.1144\n",
      "Epoch 7/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0780\n",
      "Epoch 00007: val_loss improved from 0.11437 to 0.09557, saving model to seq2seq\n",
      "2531/2531 [==============================] - 429s 169ms/step - loss: 0.0780 - val_loss: 0.0956\n",
      "Epoch 8/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0597\n",
      "Epoch 00008: val_loss improved from 0.09557 to 0.08455, saving model to seq2seq\n",
      "2531/2531 [==============================] - 428s 169ms/step - loss: 0.0597 - val_loss: 0.0846\n",
      "Epoch 9/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0469\n",
      "Epoch 00009: val_loss improved from 0.08455 to 0.07654, saving model to seq2seq\n",
      "2531/2531 [==============================] - 428s 169ms/step - loss: 0.0469 - val_loss: 0.0765\n",
      "Epoch 10/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0378\n",
      "Epoch 00010: val_loss improved from 0.07654 to 0.07184, saving model to seq2seq\n",
      "2531/2531 [==============================] - 428s 169ms/step - loss: 0.0378 - val_loss: 0.0718\n",
      "Epoch 11/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0314\n",
      "Epoch 00011: val_loss improved from 0.07184 to 0.06782, saving model to seq2seq\n",
      "2531/2531 [==============================] - 428s 169ms/step - loss: 0.0314 - val_loss: 0.0678\n",
      "Epoch 12/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0267\n",
      "Epoch 00012: val_loss improved from 0.06782 to 0.06619, saving model to seq2seq\n",
      "2531/2531 [==============================] - 428s 169ms/step - loss: 0.0267 - val_loss: 0.0662\n",
      "Epoch 13/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0233\n",
      "Epoch 00013: val_loss improved from 0.06619 to 0.06447, saving model to seq2seq\n",
      "2531/2531 [==============================] - 433s 171ms/step - loss: 0.0233 - val_loss: 0.0645\n",
      "Epoch 14/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0207\n",
      "Epoch 00014: val_loss improved from 0.06447 to 0.06300, saving model to seq2seq\n",
      "2531/2531 [==============================] - 427s 169ms/step - loss: 0.0207 - val_loss: 0.0630\n",
      "Epoch 15/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0186\n",
      "Epoch 00015: val_loss did not improve from 0.06300\n",
      "2531/2531 [==============================] - 425s 168ms/step - loss: 0.0186 - val_loss: 0.0634\n",
      "Epoch 16/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0171\n",
      "Epoch 00016: val_loss improved from 0.06300 to 0.06237, saving model to seq2seq\n",
      "2531/2531 [==============================] - 427s 169ms/step - loss: 0.0171 - val_loss: 0.0624\n",
      "Epoch 17/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0158\n",
      "Epoch 00017: val_loss did not improve from 0.06237\n",
      "2531/2531 [==============================] - 423s 167ms/step - loss: 0.0158 - val_loss: 0.0625\n",
      "Epoch 18/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0148\n",
      "Epoch 00018: val_loss did not improve from 0.06237\n",
      "2531/2531 [==============================] - 425s 168ms/step - loss: 0.0148 - val_loss: 0.0631\n",
      "Epoch 19/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00019: val_loss did not improve from 0.06237\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "2531/2531 [==============================] - 425s 168ms/step - loss: 0.0140 - val_loss: 0.0628\n",
      "Epoch 20/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0102\n",
      "Epoch 00020: val_loss improved from 0.06237 to 0.05994, saving model to seq2seq\n",
      "2531/2531 [==============================] - 428s 169ms/step - loss: 0.0102 - val_loss: 0.0599\n",
      "Epoch 21/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0090\n",
      "Epoch 00021: val_loss improved from 0.05994 to 0.05987, saving model to seq2seq\n",
      "2531/2531 [==============================] - 428s 169ms/step - loss: 0.0090 - val_loss: 0.0599\n",
      "Epoch 22/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0086\n",
      "Epoch 00022: val_loss improved from 0.05987 to 0.05984, saving model to seq2seq\n",
      "2531/2531 [==============================] - 427s 169ms/step - loss: 0.0086 - val_loss: 0.0598\n",
      "Epoch 23/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0083\n",
      "Epoch 00023: val_loss did not improve from 0.05984\n",
      "2531/2531 [==============================] - 426s 168ms/step - loss: 0.0083 - val_loss: 0.0600\n",
      "Epoch 24/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0081\n",
      "Epoch 00024: val_loss did not improve from 0.05984\n",
      "2531/2531 [==============================] - 430s 170ms/step - loss: 0.0081 - val_loss: 0.0602\n",
      "Epoch 25/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0080\n",
      "Epoch 00025: val_loss did not improve from 0.05984\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "2531/2531 [==============================] - 442s 175ms/step - loss: 0.0080 - val_loss: 0.0602\n",
      "Epoch 26/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0075\n",
      "Epoch 00026: val_loss did not improve from 0.05984\n",
      "2531/2531 [==============================] - 447s 176ms/step - loss: 0.0075 - val_loss: 0.0602\n",
      "Epoch 27/50\n",
      "2531/2531 [==============================] - ETA: 0s - loss: 0.0074\n",
      "Epoch 00027: val_loss did not improve from 0.05984\n",
      "2531/2531 [==============================] - 453s 179ms/step - loss: 0.0074 - val_loss: 0.0603\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c3ba4d7788>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Encoder_decoder(ita_vocab_size, eng_vocab_size, embedding_dim, lstm_size, max_len_ita, max_len_eng, dense_units)\n",
    "model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy')\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "callbacks = [ModelCheckpoint('seq2seq', save_best_only= True, verbose = 1),\n",
    "             TensorBoard(log_dir = log_dir, histogram_freq=1, write_graph=True),\n",
    "             EarlyStopping(patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]\n",
    "\n",
    "model.fit(x = train_dataloader, \n",
    "          steps_per_epoch = train_dataloader.__len__(),\n",
    "          validation_data = val_dataloader,\n",
    "          validation_steps = val_dataloader.__len__(),\n",
    "          epochs = 50,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pred_Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self,*params):\n",
    "        super().__init__()\n",
    "        #Create encoder object\n",
    "        self.encoder = Encoder(inp_vocab_size = params[0], embedding_size = params[2], lstm_size = params[3], input_length = params[4])\n",
    "        #Create decoder object\n",
    "        self.decoder = Decoder(out_vocab_size = params[1], embedding_size = params[2], lstm_size = params[3], input_length = params[5])\n",
    "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
    "        self.dense = Dense(params[1], activation='softmax')\n",
    "    \n",
    "    def call(self, params, training = True):\n",
    "        '''\n",
    "        A. Pass the input sequence to Encoder layer -- Return encoder_output,encoder_final_state_h,encoder_final_state_c\n",
    "        B. Pass the target sequence to Decoder layer with intial states as encoder_final_state_h,encoder_final_state_C\n",
    "        C. Pass the decoder_outputs into Dense layer \n",
    "        \n",
    "        Return decoder_outputs\n",
    "        '''\n",
    "        enc_inp = params[0]\n",
    "        initial_state = self.encoder.initialize_states(1)\n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        pred = tf.expand_dims([tknizer_eng.word_index['<sos>']], 0)\n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        all_pred = []\n",
    "        for t in range(max_len_eng):  \n",
    "            pred, dec_h,dec_c = self.decoder(pred, [dec_h, dec_c])\n",
    "            pred = self.dense(pred)\n",
    "            pred = tf.argmax(pred, axis = -1)\n",
    "            all_pred.append(pred)\n",
    "        return all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1c427daad88>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_model = pred_Encoder_decoder(ita_vocab_size, eng_vocab_size, embedding_dim, lstm_size, max_len_ita, max_len_eng, dense_units)\n",
    "pred_model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy')\n",
    "pred_model.load_weights('seq2seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_sequence):\n",
    "    seq = preprocess(input_sequence)\n",
    "    seq = '<sos> '+seq+' <eos>'\n",
    "    seq = tknizer_ita.texts_to_sequences([seq])\n",
    "    seq = pad_sequences(seq, maxlen=max_len_ita, padding='post', dtype = np.int32)\n",
    "    pred = pred_model.predict(tf.expand_dims(seq, 0))\n",
    "    output = []\n",
    "    for i in pred:\n",
    "        word = tknizer_eng.index_word[i[0][0]]\n",
    "        if word == '<eos>':\n",
    "            break\n",
    "        output.append(word)\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "zYdHl_DGgxZZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  sono brave persone\n",
      "predicted output :  they are good people\n",
      "actual output : they are good people\n"
     ]
    }
   ],
   "source": [
    "sentence = train['italin'].values[0][6:-6]\n",
    "print('input : ', sentence)\n",
    "result = predict(sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', train['english'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "xxuxoBcXeT5B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  cercherò di essere pi gentile\n",
      "predicted output :  i will try to be nicer\n",
      "actual output : I will try to be nicer\n"
     ]
    }
   ],
   "source": [
    "sentence = train['italin'].values[1000][6:-6]\n",
    "print('input : ', sentence)\n",
    "result = predict(sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', train['english'].values[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "996pFO8BLOfG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu Score : 0.38799303126017415\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "score =0\n",
    "for i in range(1000):\n",
    "    sentence = test['italin'].values[i][6:-6]\n",
    "    reference = test['english'].values[i]\n",
    "    pred = predict(sentence)\n",
    "    score+= sentence_bleu([reference.split()], pred.split())\n",
    "print('Bleu Score : {}'.format(score/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxWFDxZXLOfJ"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZhX3K9GLOfJ"
   },
   "source": [
    "## 2: Seq2Seq with Attention mechanisum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMm3ADQDLOfK"
   },
   "source": [
    "<font color='blue'>**Encoder**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "emKzXH0LgxZZ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        #Initialize Embedding layer\n",
    "        self.enc_embed = Embedding(input_dim = inp_vocab_size, output_dim = embedding_size)\n",
    "        #Intialize Encoder LSTM layer\n",
    "        self.enc_lstm = LSTM(lstm_size, return_sequences = True, return_state = True)\n",
    "        \n",
    "    def call(self,input_sequence,states):\n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "          returns -- encoder_output, last time step's hidden and cell state\n",
    "        '''\n",
    "        embedding = self.enc_embed(input_sequence)\n",
    "        output_state, enc_h, enc_c = self.enc_lstm(embedding, initial_state = states)\n",
    "        return output_state, enc_h, enc_c\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "      '''\n",
    "      Given a batch size it will return intial hidden state and intial cell state.\n",
    "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "      '''\n",
    "      return [tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXn278lhLYRM"
   },
   "source": [
    "<font color='blue'>**Attention Mechanism**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ab5SNdPZLlur"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,scoring_function, att_units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scoring_function = scoring_function\n",
    "        if scoring_function == 'dot':\n",
    "            self.dot = Dot(axes = (1, 2))\n",
    "        elif scoring_function == 'general':\n",
    "          # Intialize variables needed for General score function here\n",
    "            self.W = Dense(att_units)\n",
    "            self.dot = Dot(axes = (1, 2))\n",
    "        elif scoring_function == 'concat':\n",
    "          # Intialize variables needed for Concat score function here\n",
    "            self.W1 = Dense(att_units)\n",
    "            self.W2 = Dense(att_units)\n",
    "            self.V = Dense(1)\n",
    "    def call(self,decoder_hidden_state,encoder_output):\n",
    "    \n",
    "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 1)\n",
    "        \n",
    "        if self.scoring_function == 'dot':\n",
    "            # Implement Dot score function here\n",
    "            score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), encoder_output]), (0, 2,1))\n",
    "            \n",
    "        elif self.scoring_function == 'general':\n",
    "            # Implement General score function here\n",
    "            mul = self.W(encoder_output)\n",
    "            score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), mul]), (0, 2,1))\n",
    "            \n",
    "        elif self.scoring_function == 'concat':\n",
    "            # Implement General score function here\n",
    "            inter = self.W1(decoder_hidden_state) + self.W2(encoder_output)\n",
    "            tan = tf.nn.tanh(inter)\n",
    "            score = self.V(tan)\n",
    "        attention_weights = tf.nn.softmax(score, axis =1)\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic-FNEbfL2DN"
   },
   "source": [
    "<font color='blue'>**OneStepDecoder**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Kc8m7lmOL097"
   },
   "outputs": [],
   "source": [
    "class OneStepDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "        super(OneStepDecoder, self).__init__()\n",
    "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "        self.embed_dec = Embedding(input_dim = tar_vocab_size, output_dim = embedding_dim)\n",
    "        self.lstm = LSTM(dec_units, return_sequences = True, return_state = True)\n",
    "        self.attention = Attention(scoring_function = score_fun, att_units = att_units)\n",
    "        self.fc = Dense(tar_vocab_size)\n",
    "    \n",
    "    def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "        '''\n",
    "            One step decoder mechanisim step by step:\n",
    "          A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "          B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "          C. Concat the context vector with the step A output\n",
    "          D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "          E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "          F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "        '''\n",
    "        embed = self.embed_dec(input_to_decoder)\n",
    "        context_vect, attention_weights = self.attention(state_h, encoder_output)    \n",
    "        final_inp = tf.concat([tf.expand_dims(context_vect, 1), embed], axis = -1)\n",
    "        out, dec_h, dec_c = self.lstm(final_inp, [state_h, state_c])\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        output = self.fc(out)\n",
    "        return output, dec_h, dec_c, attention_weights, context_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FHrurjUMGAi"
   },
   "source": [
    "<font color='blue'>**Decoder**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "NV-x31rj6Hc4"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_length = input_length\n",
    "        self.out_vocab_size = out_vocab_size\n",
    "        self.one_step_decoder = OneStepDecoder(out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
    "        self.out_vocab_size = out_vocab_size\n",
    "        \n",
    "    def call(self, input_to_decoder, encoder_output, decoder_hidden_state, decoder_cell_state):\n",
    "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
    "        all_outputs = tf.TensorArray(dtype = tf.float32, size= input_to_decoder.shape[1])\n",
    "        \n",
    "        for timestep in range(input_to_decoder.shape[1]):\n",
    "            output, decoder_hidden_state, decoder_cell_state, _, _ = self.one_step_decoder(input_to_decoder[:, timestep:timestep+1], \n",
    "                                                                                             encoder_output, \n",
    "                                                                                             decoder_hidden_state,\n",
    "                                                                                             decoder_cell_state)\n",
    "            # Store the output in tensorarray\n",
    "            all_outputs = all_outputs.write(timestep, output)\n",
    "        # Return the tensor array\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0, 2))\n",
    "        return all_outputs\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC1T1EOoMTqC"
   },
   "source": [
    "<font color='blue'>**Encoder Decoder model**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "FfqBIe20MT3D"
   },
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_dim, enc_units, dec_units, max_len_ita, max_len_eng, score_fun, att_units, batch_size):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super(encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_dim, enc_units, max_len_ita)\n",
    "        self.one_step_decoder = OneStepDecoder(out_vocab_size, embedding_dim, max_len_eng, dec_units ,score_fun ,att_units)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, data):\n",
    "        enc_inp, dec_inp = data[0], data[1]\n",
    "        initial_state = self.encoder.initialize_states(self.batch_size)\n",
    "        enc_output, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        all_outputs = tf.TensorArray(dtype = tf.float32, size= 50)\n",
    "        \n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        for timestep in range(50):\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            output, dec_h, dec_c, _, _ = self.one_step_decoder(dec_inp[:, timestep:timestep+1], \n",
    "                                                               enc_output, \n",
    "                                                               dec_h,\n",
    "                                                               dec_c)\n",
    "            # Store the output in tensorarray\n",
    "            all_outputs = all_outputs.write(timestep, output)\n",
    "        # Return the tensor array\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0, 2))\n",
    "        # return the decoder output\n",
    "        return all_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVRxB-FDMJWL"
   },
   "source": [
    "<font color='blue'>**Custom loss function**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "QY_3izrXMs8y"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QlbWAqNNlqe"
   },
   "source": [
    "<font color='blue'>**Training**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqtZUQF2NuZE"
   },
   "source": [
    "Implement dot function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "lRvu9PfYyyNS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import*\n",
    "import os\n",
    "batch_size=128\n",
    "lstm_size=128\n",
    "max_len_eng = 50\n",
    "max_len_ita = 29\n",
    "embedding_dim = 100\n",
    "att_units = 256\n",
    "\n",
    "train_dataset = Dataset(train, tknizer_ita, tknizer_eng, max_len_ita, max_len_eng)\n",
    "test_dataset  = Dataset(test, tknizer_ita, tknizer_eng, max_len_ita, max_len_eng)\n",
    "val_dataset  = Dataset(validation, tknizer_ita, tknizer_eng, max_len_ita, max_len_eng)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=batch_size)\n",
    "val_dataloader = Dataloder(val_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "BJu8PxZVYJBN"
   },
   "outputs": [],
   "source": [
    "model = encoder_decoder(ita_vocab_size, eng_vocab_size, embedding_dim, lstm_size, lstm_size, max_len_ita, max_len_eng, 'dot', att_units, batch_size)\n",
    "model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "callbacks = [ModelCheckpoint('Attention_dot', save_best_only= True, verbose = 1),\n",
    "             TensorBoard(log_dir = log_dir, histogram_freq=1, write_graph=True),\n",
    "             EarlyStopping(patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  2/632 [..............................] - ETA: 36:56 - loss: 0.9827WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3979s vs `on_train_batch_end` time: 6.6300s). Check your callbacks.\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.4580\n",
      "Epoch 00001: val_loss improved from inf to 0.40010, saving model to Attention_dot\n",
      "632/632 [==============================] - 254s 403ms/step - loss: 0.4580 - val_loss: 0.4001\n",
      "Epoch 2/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.3751\n",
      "Epoch 00002: val_loss improved from 0.40010 to 0.34164, saving model to Attention_dot\n",
      "632/632 [==============================] - 239s 378ms/step - loss: 0.3751 - val_loss: 0.3416\n",
      "Epoch 3/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.3218\n",
      "Epoch 00003: val_loss improved from 0.34164 to 0.30097, saving model to Attention_dot\n",
      "632/632 [==============================] - 241s 381ms/step - loss: 0.3218 - val_loss: 0.3010\n",
      "Epoch 4/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.2860\n",
      "Epoch 00004: val_loss improved from 0.30097 to 0.27541, saving model to Attention_dot\n",
      "632/632 [==============================] - 244s 386ms/step - loss: 0.2860 - val_loss: 0.2754\n",
      "Epoch 5/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.2604\n",
      "Epoch 00005: val_loss improved from 0.27541 to 0.25371, saving model to Attention_dot\n",
      "632/632 [==============================] - 239s 379ms/step - loss: 0.2604 - val_loss: 0.2537\n",
      "Epoch 6/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.2365\n",
      "Epoch 00006: val_loss improved from 0.25371 to 0.23366, saving model to Attention_dot\n",
      "632/632 [==============================] - 246s 390ms/step - loss: 0.2365 - val_loss: 0.2337\n",
      "Epoch 7/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.2150\n",
      "Epoch 00007: val_loss improved from 0.23366 to 0.21539, saving model to Attention_dot\n",
      "632/632 [==============================] - 244s 386ms/step - loss: 0.2150 - val_loss: 0.2154\n",
      "Epoch 8/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.1949\n",
      "Epoch 00008: val_loss improved from 0.21539 to 0.19836, saving model to Attention_dot\n",
      "632/632 [==============================] - 243s 384ms/step - loss: 0.1949 - val_loss: 0.1984\n",
      "Epoch 9/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.1763\n",
      "Epoch 00009: val_loss improved from 0.19836 to 0.18322, saving model to Attention_dot\n",
      "632/632 [==============================] - 235s 373ms/step - loss: 0.1763 - val_loss: 0.1832\n",
      "Epoch 10/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.1583\n",
      "Epoch 00010: val_loss improved from 0.18322 to 0.16827, saving model to Attention_dot\n",
      "632/632 [==============================] - 244s 385ms/step - loss: 0.1583 - val_loss: 0.1683\n",
      "Epoch 11/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.1411\n",
      "Epoch 00011: val_loss improved from 0.16827 to 0.15513, saving model to Attention_dot\n",
      "632/632 [==============================] - 245s 387ms/step - loss: 0.1411 - val_loss: 0.1551\n",
      "Epoch 12/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00012: val_loss improved from 0.15513 to 0.14136, saving model to Attention_dot\n",
      "632/632 [==============================] - 242s 383ms/step - loss: 0.1249 - val_loss: 0.1414\n",
      "Epoch 13/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.1097\n",
      "Epoch 00013: val_loss improved from 0.14136 to 0.12951, saving model to Attention_dot\n",
      "632/632 [==============================] - 241s 381ms/step - loss: 0.1097 - val_loss: 0.1295\n",
      "Epoch 14/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0955\n",
      "Epoch 00014: val_loss improved from 0.12951 to 0.11921, saving model to Attention_dot\n",
      "632/632 [==============================] - 238s 377ms/step - loss: 0.0955 - val_loss: 0.1192\n",
      "Epoch 15/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0829\n",
      "Epoch 00015: val_loss improved from 0.11921 to 0.11002, saving model to Attention_dot\n",
      "632/632 [==============================] - 247s 391ms/step - loss: 0.0829 - val_loss: 0.1100\n",
      "Epoch 16/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0722\n",
      "Epoch 00016: val_loss improved from 0.11002 to 0.10264, saving model to Attention_dot\n",
      "632/632 [==============================] - 244s 386ms/step - loss: 0.0722 - val_loss: 0.1026\n",
      "Epoch 17/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0624\n",
      "Epoch 00017: val_loss improved from 0.10264 to 0.09496, saving model to Attention_dot\n",
      "632/632 [==============================] - 244s 386ms/step - loss: 0.0624 - val_loss: 0.0950\n",
      "Epoch 18/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0543\n",
      "Epoch 00018: val_loss improved from 0.09496 to 0.09032, saving model to Attention_dot\n",
      "632/632 [==============================] - 242s 382ms/step - loss: 0.0543 - val_loss: 0.0903\n",
      "Epoch 19/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0475\n",
      "Epoch 00019: val_loss improved from 0.09032 to 0.08494, saving model to Attention_dot\n",
      "632/632 [==============================] - 238s 376ms/step - loss: 0.0475 - val_loss: 0.0849\n",
      "Epoch 20/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0418\n",
      "Epoch 00020: val_loss improved from 0.08494 to 0.08147, saving model to Attention_dot\n",
      "632/632 [==============================] - 240s 379ms/step - loss: 0.0418 - val_loss: 0.0815\n",
      "Epoch 21/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0370\n",
      "Epoch 00021: val_loss improved from 0.08147 to 0.07834, saving model to Attention_dot\n",
      "632/632 [==============================] - 239s 378ms/step - loss: 0.0370 - val_loss: 0.0783\n",
      "Epoch 22/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 00022: val_loss improved from 0.07834 to 0.07500, saving model to Attention_dot\n",
      "632/632 [==============================] - 242s 384ms/step - loss: 0.0329 - val_loss: 0.0750\n",
      "Epoch 23/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00023: val_loss improved from 0.07500 to 0.07306, saving model to Attention_dot\n",
      "632/632 [==============================] - 237s 374ms/step - loss: 0.0293 - val_loss: 0.0731\n",
      "Epoch 24/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0266\n",
      "Epoch 00024: val_loss improved from 0.07306 to 0.07209, saving model to Attention_dot\n",
      "632/632 [==============================] - 249s 393ms/step - loss: 0.0266 - val_loss: 0.0721\n",
      "Epoch 25/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0241\n",
      "Epoch 00025: val_loss improved from 0.07209 to 0.06982, saving model to Attention_dot\n",
      "632/632 [==============================] - 244s 386ms/step - loss: 0.0241 - val_loss: 0.0698\n",
      "Epoch 26/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0217\n",
      "Epoch 00026: val_loss did not improve from 0.06982\n",
      "632/632 [==============================] - 239s 379ms/step - loss: 0.0217 - val_loss: 0.0699\n",
      "Epoch 27/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0199\n",
      "Epoch 00027: val_loss improved from 0.06982 to 0.06801, saving model to Attention_dot\n",
      "632/632 [==============================] - 238s 377ms/step - loss: 0.0199 - val_loss: 0.0680\n",
      "Epoch 28/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0184\n",
      "Epoch 00028: val_loss did not improve from 0.06801\n",
      "632/632 [==============================] - 240s 380ms/step - loss: 0.0184 - val_loss: 0.0680\n",
      "Epoch 29/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0170\n",
      "Epoch 00029: val_loss improved from 0.06801 to 0.06704, saving model to Attention_dot\n",
      "632/632 [==============================] - 245s 388ms/step - loss: 0.0170 - val_loss: 0.0670\n",
      "Epoch 30/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0156\n",
      "Epoch 00030: val_loss improved from 0.06704 to 0.06698, saving model to Attention_dot\n",
      "632/632 [==============================] - 244s 386ms/step - loss: 0.0156 - val_loss: 0.0670\n",
      "Epoch 31/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0147\n",
      "Epoch 00031: val_loss improved from 0.06698 to 0.06621, saving model to Attention_dot\n",
      "632/632 [==============================] - 243s 384ms/step - loss: 0.0147 - val_loss: 0.0662\n",
      "Epoch 32/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0138\n",
      "Epoch 00032: val_loss did not improve from 0.06621\n",
      "632/632 [==============================] - 237s 374ms/step - loss: 0.0138 - val_loss: 0.0662\n",
      "Epoch 33/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0129\n",
      "Epoch 00033: val_loss improved from 0.06621 to 0.06554, saving model to Attention_dot\n",
      "632/632 [==============================] - 238s 376ms/step - loss: 0.0129 - val_loss: 0.0655\n",
      "Epoch 34/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0121\n",
      "Epoch 00034: val_loss did not improve from 0.06554\n",
      "632/632 [==============================] - 243s 384ms/step - loss: 0.0121 - val_loss: 0.0666\n",
      "Epoch 35/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0116\n",
      "Epoch 00035: val_loss did not improve from 0.06554\n",
      "632/632 [==============================] - 236s 374ms/step - loss: 0.0116 - val_loss: 0.0662\n",
      "Epoch 36/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0110\n",
      "Epoch 00036: val_loss did not improve from 0.06554\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "632/632 [==============================] - 238s 376ms/step - loss: 0.0110 - val_loss: 0.0664\n",
      "Epoch 37/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0082\n",
      "Epoch 00037: val_loss improved from 0.06554 to 0.06393, saving model to Attention_dot\n",
      "632/632 [==============================] - 241s 381ms/step - loss: 0.0082 - val_loss: 0.0639\n",
      "Epoch 38/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0075\n",
      "Epoch 00038: val_loss improved from 0.06393 to 0.06381, saving model to Attention_dot\n",
      "632/632 [==============================] - 244s 387ms/step - loss: 0.0075 - val_loss: 0.0638\n",
      "Epoch 39/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 00039: val_loss did not improve from 0.06381\n",
      "632/632 [==============================] - 246s 390ms/step - loss: 0.0072 - val_loss: 0.0639\n",
      "Epoch 40/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0070\n",
      "Epoch 00040: val_loss did not improve from 0.06381\n",
      "632/632 [==============================] - 248s 392ms/step - loss: 0.0070 - val_loss: 0.0641\n",
      "Epoch 41/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0069\n",
      "Epoch 00041: val_loss did not improve from 0.06381\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "632/632 [==============================] - 243s 384ms/step - loss: 0.0069 - val_loss: 0.0641\n",
      "Epoch 42/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0065\n",
      "Epoch 00042: val_loss did not improve from 0.06381\n",
      "632/632 [==============================] - 239s 378ms/step - loss: 0.0065 - val_loss: 0.0641\n",
      "Epoch 43/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0065\n",
      "Epoch 00043: val_loss did not improve from 0.06381\n",
      "632/632 [==============================] - 238s 377ms/step - loss: 0.0065 - val_loss: 0.0641\n",
      "Epoch 00043: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c4ee3844c8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = train_dataloader, \n",
    "          steps_per_epoch = train_dataloader.__len__(),\n",
    "          validation_data = val_dataloader,\n",
    "          validation_steps = val_dataloader.__len__(),\n",
    "          epochs = 50,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pred_Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_dim, enc_units, dec_units, max_len_ita, max_len_eng, score_fun, att_units):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super(pred_Encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_dim, enc_units, max_len_ita)\n",
    "        self.one_step_decoder = OneStepDecoder(out_vocab_size, embedding_dim, max_len_eng, dec_units ,score_fun ,att_units)\n",
    "        self.batch_size = batch_size\n",
    "    def call(self, params):\n",
    "        enc_inp = params[0]\n",
    "        initial_state = self.encoder.initialize_states(1)\n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        pred = tf.expand_dims([tknizer_eng.word_index['<sos>']], 0)\n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        all_pred = []\n",
    "        all_attention = []\n",
    "        for t in range(50):  \n",
    "            pred, dec_h,dec_c, attention, _ = self.one_step_decoder(pred, output_state, dec_h, dec_c)\n",
    "            pred = tf.argmax(pred, axis = -1)\n",
    "            all_pred.append(pred)\n",
    "            pred = tf.expand_dims(pred, 0)\n",
    "            all_attention.append(attention)\n",
    "        return all_pred, all_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1c881445bc8>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_model = pred_Encoder_decoder(ita_vocab_size, eng_vocab_size, embedding_dim, lstm_size, lstm_size, max_len_ita, max_len_eng, 'dot', att_units)\n",
    "pred_model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "pred_model.load_weights('Attention_dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DpC9zlzMcXp"
   },
   "source": [
    "## <font color='blue'>**Inference**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5NhESYyMW_t"
   },
   "source": [
    "<font color='blue'>**Plot attention weights**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "pkEY7SsBMtrC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_sequence):\n",
    "    seq = preprocess(input_sequence)\n",
    "    seq = '<sos> '+seq+' <eos>'\n",
    "    seq = tknizer_ita.texts_to_sequences([seq])\n",
    "    seq = pad_sequences(seq, maxlen=max_len_ita, padding='post', dtype = np.int32)\n",
    "    pred, attention_weights = pred_model.predict(tf.expand_dims(seq, 0))\n",
    "    output = []\n",
    "    for i in pred:\n",
    "        word = tknizer_eng.index_word[i[0]]\n",
    "        if word == '<eos>':\n",
    "            break\n",
    "        output.append(word)\n",
    "    return ' '.join(output), np.squeeze(np.squeeze(np.array(attention_weights), 1), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1IhdBrgQYJr"
   },
   "source": [
    "<font color='blue'>**Predict the sentence translation**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "POxTlfNigxZa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  sono brave persone\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted output :  they are good people\n",
      "actual output : they are good people\n"
     ]
    }
   ],
   "source": [
    "sentence = train['italin'].values[0][6:-6]\n",
    "print('input : ', sentence)\n",
    "result, attention_plot = predict(sentence)\n",
    "attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', train['english'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "VoxeltFSeT5D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  cercherò di essere pi gentile\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted output :  i will try to be nicer\n",
      "actual output : I will try to be nicer\n"
     ]
    }
   ],
   "source": [
    "sentence = train['italin'].values[1000][6:-6]\n",
    "print('input : ', sentence)\n",
    "result, attention_plot = predict(sentence)\n",
    "attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', train['english'].values[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmxIVOOQPWMu"
   },
   "source": [
    "<font color='blue'>**Calculate BLEU score**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "0iHiLdROM23l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu Score : 0.37088297078207916\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "score =0\n",
    "for i in range(1000):\n",
    "    sentence = test['italin'].values[i][6:-6]\n",
    "    reference = test['english'].values[i]\n",
    "    pred, _ = predict(sentence)\n",
    "    score+= sentence_bleu([reference.split()], pred.split())\n",
    "print('Bleu Score : {}'.format(score/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWg2ferDQvT3"
   },
   "source": [
    "<font color='blue'>**Repeat the same steps for General scoring function**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "ZffDipQvgxZa"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import*\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "batch_size=128\n",
    "lstm_size=256\n",
    "embedding_dim = 100\n",
    "\n",
    "train_dataset = Dataset(train, tknizer_ita, tknizer_eng, max_len_ita, max_len_eng)\n",
    "test_dataset  = Dataset(test, tknizer_ita, tknizer_eng, max_len_ita, max_len_eng)\n",
    "val_dataset  = Dataset(validation, tknizer_ita, tknizer_eng, max_len_ita, max_len_eng)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=batch_size)\n",
    "val_dataloader = Dataloder(val_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoder_decoder(ita_vocab_size, eng_vocab_size, embedding_dim, lstm_size, lstm_size, max_len_ita, max_len_eng, 'general', att_units, batch_size)\n",
    "model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "callbacks = [ModelCheckpoint('Attention_general', save_best_only= True, verbose = 1),\n",
    "             TensorBoard(log_dir = log_dir, histogram_freq=1, write_graph=True),\n",
    "             EarlyStopping(patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  2/632 [..............................] - ETA: 42:33 - loss: 0.9890WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4269s vs `on_train_batch_end` time: 7.6672s). Check your callbacks.\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.4437\n",
      "Epoch 00001: val_loss improved from inf to 0.39343, saving model to Attention_general\n",
      "632/632 [==============================] - 291s 460ms/step - loss: 0.4437 - val_loss: 0.3934\n",
      "Epoch 2/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.3656\n",
      "Epoch 00002: val_loss improved from 0.39343 to 0.32708, saving model to Attention_general\n",
      "632/632 [==============================] - 273s 432ms/step - loss: 0.3656 - val_loss: 0.3271\n",
      "Epoch 3/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.2969\n",
      "Epoch 00003: val_loss improved from 0.32708 to 0.27086, saving model to Attention_general\n",
      "632/632 [==============================] - 273s 432ms/step - loss: 0.2969 - val_loss: 0.2709\n",
      "Epoch 4/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.2465\n",
      "Epoch 00004: val_loss improved from 0.27086 to 0.23024, saving model to Attention_general\n",
      "632/632 [==============================] - 272s 430ms/step - loss: 0.2465 - val_loss: 0.2302\n",
      "Epoch 5/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00005: val_loss improved from 0.23024 to 0.19677, saving model to Attention_general\n",
      "632/632 [==============================] - 273s 432ms/step - loss: 0.2057 - val_loss: 0.1968\n",
      "Epoch 6/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.1719\n",
      "Epoch 00006: val_loss improved from 0.19677 to 0.17108, saving model to Attention_general\n",
      "632/632 [==============================] - 274s 434ms/step - loss: 0.1719 - val_loss: 0.1711\n",
      "Epoch 7/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.1422\n",
      "Epoch 00007: val_loss improved from 0.17108 to 0.14668, saving model to Attention_general\n",
      "632/632 [==============================] - 274s 434ms/step - loss: 0.1422 - val_loss: 0.1467\n",
      "Epoch 8/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.1158\n",
      "Epoch 00008: val_loss improved from 0.14668 to 0.12656, saving model to Attention_general\n",
      "632/632 [==============================] - 274s 433ms/step - loss: 0.1158 - val_loss: 0.1266\n",
      "Epoch 9/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0934\n",
      "Epoch 00009: val_loss improved from 0.12656 to 0.10982, saving model to Attention_general\n",
      "632/632 [==============================] - 274s 433ms/step - loss: 0.0934 - val_loss: 0.1098\n",
      "Epoch 10/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0749\n",
      "Epoch 00010: val_loss improved from 0.10982 to 0.09646, saving model to Attention_general\n",
      "632/632 [==============================] - 277s 439ms/step - loss: 0.0749 - val_loss: 0.0965\n",
      "Epoch 11/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0603\n",
      "Epoch 00011: val_loss improved from 0.09646 to 0.08624, saving model to Attention_general\n",
      "632/632 [==============================] - 283s 448ms/step - loss: 0.0603 - val_loss: 0.0862\n",
      "Epoch 12/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0488\n",
      "Epoch 00012: val_loss improved from 0.08624 to 0.07903, saving model to Attention_general\n",
      "632/632 [==============================] - 279s 441ms/step - loss: 0.0488 - val_loss: 0.0790\n",
      "Epoch 13/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0400\n",
      "Epoch 00013: val_loss improved from 0.07903 to 0.07378, saving model to Attention_general\n",
      "632/632 [==============================] - 310s 491ms/step - loss: 0.0400 - val_loss: 0.0738\n",
      "Epoch 14/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0333\n",
      "Epoch 00014: val_loss improved from 0.07378 to 0.06907, saving model to Attention_general\n",
      "632/632 [==============================] - 988s 2s/step - loss: 0.0333 - val_loss: 0.0691\n",
      "Epoch 15/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0281\n",
      "Epoch 00015: val_loss improved from 0.06907 to 0.06659, saving model to Attention_general\n",
      "632/632 [==============================] - 267s 422ms/step - loss: 0.0281 - val_loss: 0.0666\n",
      "Epoch 16/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0240\n",
      "Epoch 00016: val_loss improved from 0.06659 to 0.06407, saving model to Attention_general\n",
      "632/632 [==============================] - 277s 439ms/step - loss: 0.0240 - val_loss: 0.0641\n",
      "Epoch 17/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0208\n",
      "Epoch 00017: val_loss improved from 0.06407 to 0.06257, saving model to Attention_general\n",
      "632/632 [==============================] - 283s 447ms/step - loss: 0.0208 - val_loss: 0.0626\n",
      "Epoch 18/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0184\n",
      "Epoch 00018: val_loss improved from 0.06257 to 0.06156, saving model to Attention_general\n",
      "632/632 [==============================] - 280s 444ms/step - loss: 0.0184 - val_loss: 0.0616\n",
      "Epoch 19/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0162\n",
      "Epoch 00019: val_loss improved from 0.06156 to 0.06034, saving model to Attention_general\n",
      "632/632 [==============================] - 281s 444ms/step - loss: 0.0162 - val_loss: 0.0603\n",
      "Epoch 20/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0146\n",
      "Epoch 00020: val_loss improved from 0.06034 to 0.05934, saving model to Attention_general\n",
      "632/632 [==============================] - 412s 653ms/step - loss: 0.0146 - val_loss: 0.0593\n",
      "Epoch 21/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0134\n",
      "Epoch 00021: val_loss did not improve from 0.05934\n",
      "632/632 [==============================] - 1128s 2s/step - loss: 0.0134 - val_loss: 0.0597\n",
      "Epoch 22/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0121\n",
      "Epoch 00022: val_loss did not improve from 0.05934\n",
      "632/632 [==============================] - 651s 1s/step - loss: 0.0121 - val_loss: 0.0593\n",
      "Epoch 23/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0113\n",
      "Epoch 00023: val_loss did not improve from 0.05934\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "632/632 [==============================] - 267s 423ms/step - loss: 0.0113 - val_loss: 0.0598\n",
      "Epoch 24/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0079\n",
      "Epoch 00024: val_loss improved from 0.05934 to 0.05708, saving model to Attention_general\n",
      "632/632 [==============================] - 274s 433ms/step - loss: 0.0079 - val_loss: 0.0571\n",
      "Epoch 25/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0069\n",
      "Epoch 00025: val_loss improved from 0.05708 to 0.05699, saving model to Attention_general\n",
      "632/632 [==============================] - 271s 429ms/step - loss: 0.0069 - val_loss: 0.0570\n",
      "Epoch 26/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0065\n",
      "Epoch 00026: val_loss did not improve from 0.05699\n",
      "632/632 [==============================] - 271s 429ms/step - loss: 0.0065 - val_loss: 0.0570\n",
      "Epoch 27/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0063\n",
      "Epoch 00027: val_loss did not improve from 0.05699\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "632/632 [==============================] - 269s 426ms/step - loss: 0.0063 - val_loss: 0.0570\n",
      "Epoch 28/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 00028: val_loss did not improve from 0.05699\n",
      "632/632 [==============================] - 274s 433ms/step - loss: 0.0058 - val_loss: 0.0570\n",
      "Epoch 29/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 00029: val_loss did not improve from 0.05699\n",
      "632/632 [==============================] - 269s 426ms/step - loss: 0.0058 - val_loss: 0.0570\n",
      "Epoch 30/50\n",
      "632/632 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 00030: val_loss did not improve from 0.05699\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "632/632 [==============================] - 269s 425ms/step - loss: 0.0058 - val_loss: 0.0570\n",
      "Epoch 00030: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c6e4900248>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = train_dataloader, \n",
    "          steps_per_epoch = train_dataloader.__len__(),\n",
    "          validation_data = val_dataloader,\n",
    "          validation_steps = val_dataloader.__len__(),\n",
    "          epochs = 50,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1c84b9a06c8>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_model = pred_Encoder_decoder(ita_vocab_size, eng_vocab_size, embedding_dim, lstm_size, lstm_size, max_len_ita, max_len_eng, 'general', att_units)\n",
    "pred_model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "pred_model.load_weights('Attention_general')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Z8lySZDoeT5D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  sono brave persone\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output :  they are good people\n"
     ]
    }
   ],
   "source": [
    "sentence = train['italin'].values[0][6:-6]\n",
    "print('input : ', sentence)\n",
    "result, attention_plot = predict(sentence)\n",
    "attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "print('output : ',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "qJ7dSp6aeT5D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  cercherò di essere pi gentile\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output :  i will try to be nicer\n"
     ]
    }
   ],
   "source": [
    "sentence = train['italin'].values[1000][6:-6]\n",
    "print('input : ', sentence)\n",
    "result, attention_plot = predict(sentence)\n",
    "attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "print('output : ',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "PJyunDXMgxZa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu Score : 0.39123319557452246\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "score =0\n",
    "for i in range(1000):\n",
    "    sentence = test['italin'].values[i][6:-6]\n",
    "    reference = test['english'].values[i]\n",
    "    pred, _ = predict(sentence)\n",
    "    score+= sentence_bleu([reference.split()], pred.split())\n",
    "print('Bleu Score : {}'.format(score/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VB1jRUqZQ9AM"
   },
   "source": [
    "<font color='blue'>**Repeat the same steps for Concat scoring function**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "1kN9ZWViQNMB"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import*\n",
    "import os\n",
    "batch_size=64\n",
    "lstm_size=128\n",
    "embedding_dim = 100\n",
    "\n",
    "train_dataset = Dataset(train, tknizer_ita, tknizer_eng, max_len_ita, max_len_eng)\n",
    "test_dataset  = Dataset(test, tknizer_ita, tknizer_eng, max_len_ita, max_len_eng)\n",
    "val_dataset  = Dataset(validation, tknizer_ita, tknizer_eng, max_len_ita, max_len_eng)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=batch_size)\n",
    "val_dataloader = Dataloder(val_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "ElJteT5dgxZa"
   },
   "outputs": [],
   "source": [
    "model = encoder_decoder(ita_vocab_size, eng_vocab_size, embedding_dim, lstm_size, lstm_size, max_len_ita, max_len_eng, 'concat', att_units, batch_size)\n",
    "model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "callbacks = [ModelCheckpoint('Attention_concat', save_best_only= True),\n",
    "             TensorBoard(log_dir = log_dir, histogram_freq=1, write_graph=True),\n",
    "             EarlyStopping(patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "   2/1265 [..............................] - ETA: 2:27:46 - loss: 0.9803WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3760s vs `on_train_batch_end` time: 13.6508s). Check your callbacks.\n",
      "1265/1265 [==============================] - 476s 376ms/step - loss: 0.4247 - val_loss: 0.3481\n",
      "Epoch 2/50\n",
      "1265/1265 [==============================] - 455s 359ms/step - loss: 0.3086 - val_loss: 0.2701\n",
      "Epoch 3/50\n",
      "1265/1265 [==============================] - 450s 356ms/step - loss: 0.2430 - val_loss: 0.2236\n",
      "Epoch 4/50\n",
      "1265/1265 [==============================] - 459s 363ms/step - loss: 0.2004 - val_loss: 0.1920\n",
      "Epoch 5/50\n",
      "1265/1265 [==============================] - 455s 360ms/step - loss: 0.1661 - val_loss: 0.1641\n",
      "Epoch 6/50\n",
      "1265/1265 [==============================] - 454s 359ms/step - loss: 0.1361 - val_loss: 0.1415\n",
      "Epoch 7/50\n",
      "1265/1265 [==============================] - 453s 358ms/step - loss: 0.1100 - val_loss: 0.1216\n",
      "Epoch 8/50\n",
      "1265/1265 [==============================] - 453s 358ms/step - loss: 0.0881 - val_loss: 0.1058\n",
      "Epoch 9/50\n",
      "1265/1265 [==============================] - 456s 360ms/step - loss: 0.0707 - val_loss: 0.0941\n",
      "Epoch 10/50\n",
      "1265/1265 [==============================] - 453s 358ms/step - loss: 0.0572 - val_loss: 0.0845\n",
      "Epoch 11/50\n",
      "1265/1265 [==============================] - 453s 358ms/step - loss: 0.0468 - val_loss: 0.0781\n",
      "Epoch 12/50\n",
      "1265/1265 [==============================] - 450s 355ms/step - loss: 0.0390 - val_loss: 0.0735\n",
      "Epoch 13/50\n",
      "1265/1265 [==============================] - 451s 357ms/step - loss: 0.0330 - val_loss: 0.0698\n",
      "Epoch 14/50\n",
      "1265/1265 [==============================] - 449s 355ms/step - loss: 0.0283 - val_loss: 0.0673\n",
      "Epoch 15/50\n",
      "1265/1265 [==============================] - 452s 357ms/step - loss: 0.0245 - val_loss: 0.0661\n",
      "Epoch 16/50\n",
      "1265/1265 [==============================] - 451s 356ms/step - loss: 0.0217 - val_loss: 0.0645\n",
      "Epoch 17/50\n",
      "1265/1265 [==============================] - 452s 357ms/step - loss: 0.0192 - val_loss: 0.0638\n",
      "Epoch 18/50\n",
      "1265/1265 [==============================] - 451s 356ms/step - loss: 0.0174 - val_loss: 0.0626\n",
      "Epoch 19/50\n",
      "1265/1265 [==============================] - 450s 356ms/step - loss: 0.0158 - val_loss: 0.0629\n",
      "Epoch 20/50\n",
      "1265/1265 [==============================] - 452s 357ms/step - loss: 0.0145 - val_loss: 0.0618\n",
      "Epoch 21/50\n",
      "1265/1265 [==============================] - 450s 356ms/step - loss: 0.0134 - val_loss: 0.0621\n",
      "Epoch 22/50\n",
      "1265/1265 [==============================] - 451s 357ms/step - loss: 0.0125 - val_loss: 0.0625\n",
      "Epoch 23/50\n",
      "1265/1265 [==============================] - ETA: 0s - loss: 0.0117\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1265/1265 [==============================] - 448s 354ms/step - loss: 0.0117 - val_loss: 0.0635\n",
      "Epoch 24/50\n",
      "1265/1265 [==============================] - 453s 358ms/step - loss: 0.0083 - val_loss: 0.0599\n",
      "Epoch 25/50\n",
      "1265/1265 [==============================] - 457s 362ms/step - loss: 0.0072 - val_loss: 0.0598\n",
      "Epoch 26/50\n",
      "1265/1265 [==============================] - 452s 357ms/step - loss: 0.0069 - val_loss: 0.0597\n",
      "Epoch 27/50\n",
      "1265/1265 [==============================] - 449s 355ms/step - loss: 0.0066 - val_loss: 0.0599\n",
      "Epoch 28/50\n",
      "1265/1265 [==============================] - 449s 355ms/step - loss: 0.0064 - val_loss: 0.0600\n",
      "Epoch 29/50\n",
      "1265/1265 [==============================] - ETA: 0s - loss: 0.0063\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1265/1265 [==============================] - 448s 354ms/step - loss: 0.0063 - val_loss: 0.0602\n",
      "Epoch 30/50\n",
      "1265/1265 [==============================] - 448s 354ms/step - loss: 0.0058 - val_loss: 0.0602\n",
      "Epoch 31/50\n",
      "1265/1265 [==============================] - 449s 355ms/step - loss: 0.0058 - val_loss: 0.0603\n",
      "Epoch 00031: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c789245988>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = train_dataloader, \n",
    "          steps_per_epoch = train_dataloader.__len__(),\n",
    "          validation_data = val_dataloader,\n",
    "          validation_steps = val_dataloader.__len__(),\n",
    "          epochs = 50,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1c7d6aa0208>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_model = pred_Encoder_decoder(ita_vocab_size, eng_vocab_size, embedding_dim, lstm_size, lstm_size, max_len_ita, max_len_eng, 'concat', att_units)\n",
    "pred_model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "pred_model.load_weights('Attention_concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "5TYUGgxAeT5D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  sono brave persone\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAJxCAYAAADoyQNYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAcj0lEQVR4nO3dedBkB1nv8d+TBQhguKKirAbCIvuSCKJAkKjBpRAQUQEVUaOIpYAIAhevS7mwCSIiRFavgSuorFdBwRCFG8HIJsZiCbKEKGsghBCSkOf+0T2V13EmMxPyPqeH9/Opemtmzunu83SqZ77ps3RXdwcA2F6HLD0AAOwEggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXgANWVd9dVa+pqjOq6vrrZT9VVccvPdumElwADkhVPTDJS5O8L8kNkxy+XnVokkcvNdemE1wADtSjk/x0dz8iycVblv9jktstM9LmE1wADtRNkpy2h+XnJTlyeJaDhuACcKDOTnLTPSy/W5Izh2c5aAguAAfqpCTPqKpvW//5+lX140melOSPlhtrs5UvoAfgQFXVbyV5RJKrrBd9MclTuvsJy0212QQXgMulqq6a5BZZ7S09o7vPW3ikjSa4ADDgsKUHAODgUlVXSfKLSY5Pcq3sdj5Qd99mibk2neACcKCeleQ+SV6W5P8lsat0P9ilDMABqapPJ7l/d79+6VkOJi4LAuBAnZ/kI0sPcbARXAAO1JOSPLKqNOQA2KUMwAGpqlcnuWuSzyY5I8lFW9d3972WmGvTOWkKgAP1ySQvX3qIg413uMBeVdWxSY5O8pru/nxVXS3JF7v74n3cFdiNd7jAf1NVX5/kVUm+OatLPm6S5ANJfi/JBVldg8kOV1U3yuqTpjrJv3X3BxYeaaM54A3sydOS/GeSr8nqjNRdXpbkuxaZiI1RVUdW1cuSvD/JK5K8Msn7quqlVfVVy063uQQX2JPjkzy+u8/ZbfmZSW6wwDxslt9Pcpsk357kiPXP8etlT19wro0muMCeHJHkwj0s/7qsdimzs90ryU9196ndfdH6541JTkxy72VH21yCC+zJ3yd58JY/d1UdmuQxSd6wyERskiOSfGoPyz+dS7+uj904S3kHq6p75NITHs7o7lMWHokNUVW3SHJqknckOS7Ja5LcMsk1knxbd5+54HgsrKr+Nsm5SX60u89fL7takj9JcmR3f+eS820qwd2Bquq6WV1Dd0ySs9eLr5Pk9CT36e6z93Zfdo6qunaSn83qdXJIkrcl+cPu/o9FB2NxVXWrJK9NcrUk78rqf9pvm+TzSU7o7n9dcLyNJbg7UFX9RVaBfUB3//t62Y2S/GmSs7v7fkvOx/Kq6tDu/tLSc7C5quqIJA9K8k1JKqtPnDq5u7+w6GAbTHB3oKo6N8ndu/ttuy0/Nskbuvsay0zGpqiqTyR5SZI/7e63Lj0PfCVw0hRbXbL0AGyMx2d1icdpVfXeqvrVqjp66aHYDFV1/6r6ri1//tWqOquqXrc+FMEeCO7O9IYkz6iq6+9aUFU3yOraOmegku4+qbvvnuQbkzwvyQ9k9cEGp1XVzy06HJvg13b9pqrukORxSZ6R5PAkT11opo1nl/IOtA7tK5PcOquTpjrJdbM6+eH7u/usBcdjQ63/YX1ektt096FLz8NyqurzSW7R3R+qqt9McpPu/uGqul2S13X31y884kbyWco7UHd/JMkdquo7s+WEh+5+/bKTsYmq6i5JHpjkB5NcKauT69jZLkiy6yMcj0/y/PXvP7tlObvxDhf4b6rqlllF9key2vvx+qxC+3JnoVJVr8zqAy7elOQJSY7q7rOr6oQkz+jumy064IYS3B2qqu6U1f+ZXiu7Hcvv7l9YZCg2RlVdkuSfkpyc5CXd/YmFR2KDVNX1kjwrq2P8v9/dz18vf3qSQ/wbsmeCuwNV1aOSPCmrb/rYdQx3l+7ueywyGBujqm7S3e9beg42T1UdltU3Rr21uz+59DwHE8HdgarqI0me2N3PXHoW4OBTVRck+abu/uDSsxxMXBa0Mx2Z5K+WHoLNVVVXqqpfX1+De0FVfWnrz9Lzsbh3Jrnx0kMcbAR3Z3pJknsuPQQb7TeT/HhW11RekuSXk/xhVt8Q4zpcfi3JU6vq3lV1/aq65tafpYfbVHYp70BV9fgkD0/yN1lde3vR1vXd/XtLzMXmqKp/T/LQ7n5tVX0uye26+8yqemiS433e9s62Pqlul60RqazOA3Gd9h4I7g60/sd0b7q7bzQ2DBupqs7P6hjdh6vqP5J8X3f/c1XdMMk7u/vIhUdkQVV13GWt7+5Tp2Y5mPjgix2ou2+49AxsvA9n9Y1SH87qbPYTkvxzkjsncR3uDieol49juDtcVV19/cXRsNXLs7pOO1l9xvavr/eMvDDJc5cais1RVbeuqmdW1V/v+sKC9THd2y8926ayS3mHqqqHJXlMVp8ilCRnZXWp0LOWm4pNtf6glG9L8t7ufs3S87Cs9TcFvSrJXyf5niQ37+4PVNUvJblrd9970QE3lODuQFX1uCSPTfKUrD6aLUnumuSRSX67u393qdlYXlUdntXHOD6uu89ceh42T1W9JcmLuvtZ65PqbrsO7jFJXt3d11l4xI0kuDtQVX04yWO6+yW7LX9gVsH9xmUmY1NU1TlJjunuDyw9C5unqs5Lcqvu/uBuwb1hkn/r7qssPOJGcgx3Z7pWVp+Tu7u3JvG1WiTJXya579JDsLHOyaWHo7a6Q1aHp9gDZynvTO9N8oAkv7Hb8gckec/8OGygDyf5n1V11ySnJ/n81pWu1d7xXpzkyVV1/6yuwz1sfanQU5K8YNHJNphdyjtQVd03yUuTvDHJm7P6C3OXJHdPcr/ufsViw7ERXKvNZVkf539hkh/O6sMuLslqj+nJSR7c3T7+cw8Ed4dan9zwiCQ3z+ovzL8m+b3ufvuig7FxqurqSdLd5y09C5ulqm6U1f+sd5LTuvv9C4+00RzD3YGq6hZJzuvuB3X3MUl+Javo3rOqfCQbSZKqevj6BLvPJvlsVX2kqh5RVbX0bCyvqh6e1V6yF2T1bvcUr4/L5hjuzvS8rD7M4D3rL5J+eZJTkzwsq28SeuyCs7EBqupJSU5M8uQkp60X3znJrya5dpJHLzQaG8Dr4/KxS3kHqqrPJLljd7+3qh6R5F7d/e1V9e1JXtDdRy07IUurqk8nObG7/3y35fdL8pzu/pplJmMTeH1cPnYp70yHJrlw/fvjc+l3454ZlwVxqXftZZl/N0i8Pg6Y/zA707uTPHR9ycfxSV67Xn7dJJ9cbCo2yZ9kdYhhdw9N8r+HZ2HzeH1cDo7h7kyPSfKKJI/K6uPZ/mW9/F5ZffgFO1BVPWPLHw9L8qCqOiHJP66X3SmrbxA6eXo2Ns6Vkzxgb6+Pra+l7v6FBebbSI7h7lDrs5GP7O5ztiw7Ksn53f3xpeZiOVV1yn7etLv7Hts6DBvNa+XyEVwAGOAYLgAMEFwAGCC4JEmq6sSlZ2BzeX2wN14b+09w2cVfGi6L1wd747WxnwQXAAbs2LOUr3TYVfuIw6+x9Bgb48IvnZ8rHXrVpcfYGP3FC/d9ox3kor4gh9dVlh5jYxx6U9/xscsXP/OFXPl/HLH0GBvj/P/8XL74mS/s8QscduwHXxxx+DVy56MfsvQYbKhL3vfBpUdgg139eUcuPQIb6pSf/Iu9rrNLGQAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwYJHgVtXdq6qr6muX2D4ATBsJblW9saqeObEtANhEdikDwIBtD25VvTDJcUkett6N3EmOWq++bVW9parOr6rTq+oOu933W6vq1PX6j1bVH1XVket1P1ZVn6qqK+92n5Or6lXb/bwA4EBMvMP9xSSnJXlBkmuvfz6yXvc7SX4lyR2SfCrJyVVVSVJVt07yN0leleS2Se6b5HZJnr++78vW83//rg1V1TWS3CfJ87b1GQHAATpsuzfQ3Z+tqguTnN/d/5kkVfVN69VP6O5T1st+I8mbklw3yVlJfjnJn3X3U3c9VlU9NMnbq+pa3f3xqjo5yUOSvHR9kwckOTfJ/93u5wUAB2Lbg7sP79ry+7PXv14rq+Aek+TGVfVDW25T61+PTvLxJH+c5G1Vdb3uPiur+L6ouy/e08aq6sQkJybJVQ4/8gp7EgCwL0sH96Itv+/1r4ds+fW5SZ62h/t9NEm6+51V9bYkD66qVyQ5NsmD9rax7j4pyUlJco0jrt17ux0AXNGmgnthkkMP8D5vS3LL7n7/Pm73x0keneRrk7y5u99zOeYDgG01dVnQB5PcsaqOWn/Yxf5s94nr+zy7qm5fVTeuqu+rqufsdruXJPmGJA+Nk6UA2FBTwX1KVu9yz0jyiSQ32NcduvtdSe6W1SVEpyZ5Z1ZnNX9st9t9LquTpi7MpSdPAcBGGdml3N3vTXLn3Ra/cLfbfDCXnhS1a9npSe65H5u4dpL/092fv/xTAsD2WfqkqS9LVV0zyXck+a6srtUFgI10UAc3qxOrrpnkcd397qWHAYC9OaiD291HLT0DAOwPX14AAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGHLb0AIu5pFPnX7D0FGyovviipUdgg53/kCOXHoENdcmH9v4+1jtcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADDhog1tVhy89AwDsr40JblXds6r+oarOqapPV9Xrqurm63VHVVVX1Y9U1d9V1ReS/Mx63bdW1alVdX5VfbSq/qiqjlz0yQDAbjYmuEmuluTpSe6Y5O5JPpvk1VV1pS23+Z0kz0pyiySvqKpbJ/mbJK9Kctsk901yuyTPnxsbAPbtsKUH2KW7/2Lrn6vqJ5Kcm1WAz1ov/oPu/vMtt/ntJH/W3U/dsuyhSd5eVdfq7o9v/+QAsG8b8w63qo6uqhdX1ZlVdW6Sj2U13w223Oz03e52TJIHVdV5u36SvHm97ug9bOPEqjq9qk6/8Evnb8fTAIA92ph3uEleneSjWR2b/WiSi5OckWTrLuXP73afQ5I8N8nT9vB4H919QXeflOSkJLnGlb+hv/yRAWD/bERwq+prktw8ycO6+5T1sjtk3/O9Lcktu/v92zwiAHxZNmWX8jlJPpnkp6vqxlV1XJJnZ/Uu97I8Mckdq+rZVXX79X2/r6qes90DA8CB2IjgdvclSX4oyW2SvDvJHyZ5QpIv7uN+70pytyRHJTk1yTuzOpP5Y9s4LgAcsI3YpZwk3f13SW612+Krb/l97eV+pye553bNBQBXhI14hwsAX+kEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAwQHABYIDgAsAAwQWAAYILAAMEFwAGCC4ADBBcABgguAAw4LClB1hKX3RhvnTW2UuPwabqXnoCNthfnfqXS4/AhrrjCZ/Z6zrvcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAM+IoLblW9u6p+bek5AGCrr7jgAsAmElwAGLBtwa2qq1XVn1TVeVX1sap6bFW9pqpeuF7/1VX1oqo6p6q+UFWvr6pb7vYY962qf6mqL1bVR6rq8VVVW9Zfq6peub7/h6rqIdv1fADgy7Gd73CfmuS4JPdJco8kt01y1y3rX5jkTkm+P8kdk5yf5LVVdUSSVNUxSV6W5C+T3DrJryR5bJKf3+0xbpzkO5LcO8mPJTlqe54OAFx+h23Hg1bV1ZM8JMmPdfffrpf9ZJKz1r+/SZJ7JTmuu/9+vexHk3w4yQOTPDfJI5Oc2t3/a/2w713f7zFJ/qCqbprku5PcpbvfvH6MH0/ygcuY68QkJybJVXLVK/Q5A8Bl2a53uEcnOTzJW3ct6O7PJ3n3+o83T3JJktO2rP9skn9Jcostt3nzbo/7piTXraojtzzG1m18KMnZexuqu0/q7mO7+9jD68qX75kBwOWwXcHddZy197F+T3rLbfZ2/97HYwDARtmu4L4/yUVZHZtNklTVVZPcav3HM9bbvvOW9Udmdaz2jC23uctuj3uXJGd19+eS/Nv6Mb55y2PcIMl1rsgnAgBXhG0Jbnefl+T5SZ5YVcdX1S2yOi57yGp1vy/JK5M8p6ruWlW3TvKnSc5N8uL1wzw1yXFV9WtVddOqemCSX0rypPU23pPktevHuHNV3S6rk6i+sB3PCQC+HNt5lvKjkvxDklclOSXJu5KcnuSC9fqfyOr466vWv141yT27+wtJ0t1vS/KDSX4gq2O/v7v+eeaWbTw4yb8n+bskr84q1h/cvqcEAJdPde/tMOkVvKGqKyf5UJInd/dTRzZ6GY485Jr9LYedsPQYbKi++OKlR2CDve7sdyw9Ahvqjid8JKe/84I9nmO0LZcFJUlV3T6rM4nfmuSrsrqc56uS/Nl2bRMANtW2BXftkUluluTiJO9IcrfuPmubtwkAG2fbgtvdb09y7HY9PgAcTHx5AQAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGHDY0gMsppO++OKlpwAOQidc53ZLj8CGem9/aq/rvMMFgAGCCwADBBcABgguAAwQXAAYILgAMEBwAWCA4ALAAMEFgAGCCwADBBcABgguAAwQXAAYILgAMEBwAWCA4ALAAMEFgAGCCwADBBcABgguAAwQXAAYILgAMEBwAWCA4ALAAMEFgAGCCwADBBcABgguAAwQXAAYILgAMEBwAWCA4ALAAMEFgAGCCwADBBcABgguAAwQXAAYILgAMEBwAWCA4ALAAMEFgAGCCwADBBcABgguAAwQXAAYILgAMEBwAWCA4ALAAMEFgAGCCwADBBcABgguAAwQXAAYILgAMEBwAWCA4ALAAMEFgAGCCwADBBcABgguAAwQXAAYILgAMOCgCW5VvbGqnrn0HABweRw0wQWAg5ngAsCAfQZ3vSv32VX1+1V1zvrnyVV1yHr9larqiVV1VlV9vqr+qapO2O0x7lZVb6mqC6rqY1X1tKq60v5uYy9z7XO7ALAp9vcd7gPXt71zkp9JcmKSh6/XvSDJcUkekOTWSV6U5NVVddskqarrJvnrJG9PcvskP5nkR5L8zgFsY08uc7sAsEmquy/7BlVvTHKdJDfr9Y2r6n8m+dmsgve+JEd194e33OcVSc7u7p+rqt9K8kNJbtrdl6zXPzjJc5J8dXeff1nb6O7rbZnj3d3981V19L62u5fncmJWIc9VctVj7lLfs7//nQBgn97Sb8i5/ena07r9fYf7j/1fy3xakusmuUuSSnJGVZ236yfJ9yY5en3bmyc5bVds196U5EpJbryvbVTVkXuY5w77sd3/prtP6u5ju/vYw3Pl/XneAHCFOOwKeIxO8s1JLtpt+RfWv9b6Nnu77+VxyH5sFwA2xv4G905VVVvegX5LkrOzehdaSb6hu0/Zy33PSHL/qjpky7vcuyS5MMmZ+9pGd5+7h8d8+35sFwA2xv7uUr5OkqdX1c2q6n5JfjnJ07r7vUlOTvLCqrpfVd2oqo6tqkdV1X3X933W+v7PqqqbV9X3JvndJM/s7vP3tY09DbOf2wWAjbG/73BPTnJokrdktSv3ebk0hj+R5PFJnpTkekk+neStSU5Jku7+aFV9d5InJ3lHks8keXGSxx3ANvbkMrcLAJtkf89Sfnd3//y2DTGwjd0dWdfsO9XxU5sDYAe4Is5SBgC+DIILAAP2eQy3u+++3UNMbAMAluQdLgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGCC4ADBAcAFggOACwADBBYABggsAAwQXAAYILgAMEFwAGFDdvfQMi6iqTyT50NJzbJCvTfLJpYdgY3l9sDdeG//VN3b31+1pxY4NLv9VVZ3e3ccuPQebyeuDvfHa2H92KQPAAMEFgAGCyy4nLT0AG83rg73x2thPjuECwADvcAFggOACwADBBYABggsAAwQXAAb8f6sgzvxgSKygAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output :  they are good people\n"
     ]
    }
   ],
   "source": [
    "sentence = train['italin'].values[0][6:-6]\n",
    "print('input : ', sentence)\n",
    "result, attention_plot = predict(sentence)\n",
    "attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "print('output : ',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "Itdm3pSMeT5D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  cercherò di essere pi gentile\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output :  i will try to be nicer\n"
     ]
    }
   ],
   "source": [
    "sentence = train['italin'].values[1000][6:-6]\n",
    "print('input : ', sentence)\n",
    "result, attention_plot = predict(sentence)\n",
    "attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "print('output : ',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "c0Y90zVUgxZa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu Score : 0.39759090788619267\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "score =0\n",
    "for i in range(1000):\n",
    "    sentence = test['italin'].values[i][6:-6]\n",
    "    reference = test['english'].values[i]\n",
    "    pred, _ = predict(sentence)\n",
    "    score+= sentence_bleu([reference.split()], pred.split())\n",
    "print('Bleu Score : {}'.format(score/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiNOcr30gxZa"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff1lV0ITM6_p"
   },
   "source": [
    "> 1. Attention mechanism with score function as 'concat' converges faster than score function of 'general' and 'dot'.\n",
    "> 2. concat convergence > general convergence > dot convergence"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Seq2SeqImplementation__Assignment-Copy2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
